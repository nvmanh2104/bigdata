version: '3.6'
services:
    hadoop-master:
        image: ngovanmanh/hadoop:3.2.1       
        entrypoint: ['./entrypoint.sh', 'master']
        ports:
            - 50070:50070
            - 60010:60010
            - 8088:8088
            - 9870:9870
            - 6660:6660

            - 9090:8080
            - 7077:7077
        hostname: hadoop-master
        environment:
            - SPARK_NO_DAEMONIZE=true    
            - 
        # depends_on:
        #     - hadoop-slave1
        #     - hadoop-slave2
        #     - hadoop-slave3
        #     - hadoop-slave4
        #     - hadoop-slave5     
        deploy:
            restart_policy:
                condition: on-failure
            replicas: 1
            placement:
                constraints:
                    - node.hostname==kttv1
        networks:
            - hadoop
        
        volumes:
            - namenode:/root/hdfs/namenode
            - spark-master-logs:/opt/spark/spark-events
            - spark-master-zookeeper:/usr/local/zookeeper

    hadoop-secondary:
        image: ngovanmanh/hadoop:3.2.1       
        entrypoint: ['./entrypoint.sh', 'master']
        ports:
            - 51070:50070
            - 61010:60010
            - 8188:8088
            - 9170:9870
            - 6160:6660

            - 9190:8080
            - 7177:7077
        hostname: hadoop-secondary
        environment:
            - SPARK_NO_DAEMONIZE=true              
        deploy:
            restart_policy:
                condition: on-failure
            replicas: 1
            placement:
                constraints:
                    - node.hostname==kttv2
        networks:
            - hadoop
        
        volumes:
            - namenode:/root/hdfs/namenode
            - spark-master-logs:/opt/spark/spark-events
            - spark-master-zookeeper:/usr/local/zookeeper

    hadoop-slave1:
        image: ngovanmanh/hadoop:3.2.1      
        entrypoint: ['./entrypoint.sh', 'worker'] 
        ports:
            - 50071:50070
            - 6001:60010
            - 8081:8088
            - 9871:9870
            - 6661:6660
        hostname: hadoop-slave1
        environment:
            - SPARK_NO_DAEMONIZE=true    
        depends_on:
            - hadoop-master
            - hadoop-secondary
        deploy:
            restart_policy:
                condition: on-failure
            replicas: 1
            placement:
                constraints:
                    - node.hostname==kttv1
        networks:
            - hadoop
        
        volumes:
            - datanode1:/root/hdfs/datanode
            - spark-node1-logs:/opt/spark/spark-events

    hadoop-slave2:
        image: ngovanmanh/hadoop:3.2.1  
        entrypoint: ['./entrypoint.sh', 'worker']     
        ports:
            - 50072:50070
            - 60012:60010
            - 8082:8088
            - 9872:9870
            - 6662:6660
        hostname: hadoop-slave2
        environment:
            - SPARK_NO_DAEMONIZE=true
        depends_on:
            - hadoop-master   
            - hadoop-secondary
        deploy:
            restart_policy:
                condition: on-failure
            replicas: 1
            placement:
                constraints:
                    - node.hostname==kttv2
        networks:
            - hadoop
        
        volumes:
            - datanode2:/root/hdfs/datanode
            - spark-node2-logs:/opt/spark/spark-events

    hadoop-slave3:
        image: ngovanmanh/hadoop:3.2.1   
        entrypoint: ['./entrypoint.sh', 'worker']    
        ports:
            - 50073:50070
            - 60013:60010
            - 8083:8088
            - 9873:9870
            - 6663:6660
        hostname: hadoop-slave3
        environment:
            - SPARK_NO_DAEMONIZE=true   
        depends_on:
            - hadoop-master 
            - hadoop-secondary
        deploy:
            restart_policy:
                condition: on-failure
            replicas: 1
            placement:
                constraints:
                    - node.hostname==kttv2
        networks:
            - hadoop
        
        volumes:
            - datanode3:/root/hdfs/datanode
            - spark-node3-logs:/opt/spark/spark-events

    # hadoop-slave4:
    #     image: ngovanmanh/hadoop:3.2.1       
    #     ports:
    #         - 50074:50070
    #         - 60014:60010
    #         - 8084:8088
    #         - 9874:9870
    #         - 6664:6660
    #     hostname: hadoop-slave4
    #     #environment:
            
    #     deploy:
    #         restart_policy:
    #             condition: on-failure
    #         replicas: 1
    #         placement:
    #             constraints:
    #                 - node.hostname==kttv2
    #     networks:
    #         - hadoop
        
    #     volumes:
    #         - datanode4:/root/hdfs/datanode

    # hadoop-slave5:
    #     image: ngovanmanh/hadoop:3.2.1     
    #     ports:
    #         - 50075:50070
    #         - 60015:60010
    #         - 8085:8088
    #         - 9875:9870
    #         - 6665:6660
    #     hostname: hadoop-slave5
    #     #environment:
            
    #     deploy:
    #         restart_policy:
    #             condition: on-failure
    #         replicas: 1
    #         placement:
    #             constraints:
    #                 - node.hostname==kttv2
    #     networks:
    #         - hadoop
        
    #     volumes:
    #         - datanode5:/root/hdfs/datanode            

networks:
    hadoop:
        external: true

volumes:
    namenode:   
        driver: local     
        driver_opts:
            type: none
            o: bind
            device: "/opt/bigdata/namenode" 
    
    datanode1:   
        driver: local     
        driver_opts:
            type: none
            o: bind
            device: "/opt/bigdata/datanode1"
    
    datanode2:   
        driver: local     
        driver_opts:
            type: none
            o: bind
            device: "/opt/bigdata/datanode2"     

    datanode3:   
        driver: local     
        driver_opts:
            type: none
            o: bind
            device: "/opt/bigdata/datanode3"   

    spark-master-logs:   
        driver: local     
        driver_opts:
            type: none
            o: bind
            device: "/opt/bigdata/spark/spark-master-log" 

    spark-node1-logs:   
        driver: local     
        driver_opts:
            type: none
            o: bind
            device: "/opt/bigdata/spark/spark-node1-log"     

    spark-node2-logs:   
        driver: local     
        driver_opts:
            type: none
            o: bind
            device: "/opt/bigdata/spark/spark-node2-log" 

    spark-node3-logs:   
        driver: local     
        driver_opts:
            type: none
            o: bind
            device: "/opt/bigdata/spark/spark-node3-log"

    spark-master-zookeeper:   
        driver: local     
        driver_opts:
            type: none
            o: bind
            device: "/opt/bigdata/spark/zookeeper" 
    

    
    # datanode4:   
    #     driver: local     
    #     driver_opts:
    #         type: none
    #         o: bind
    #         device: "/opt/bigdata/datanode4"    

    # datanode5:   
    #     driver: local     
    #     driver_opts:
    #         type: none
    #         o: bind
    #         device: "/opt/bigdata/datanode5" 